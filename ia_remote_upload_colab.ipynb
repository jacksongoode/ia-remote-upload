{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacksongoode/ia-remote-upload/blob/main/ia_remote_upload_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4PlyWBGRzSRM"
      },
      "outputs": [],
      "source": [
        "# @title Install requirements\n",
        "%pip install internetarchive setuptools tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEU_yxKMAyvt"
      },
      "source": [
        "Please add your access key and secret key from your `ia.ini` file that should be in your home directory after you've configured the `ia` utility.\n",
        "\n",
        "The number of **workers** corresponds to the number of threads and active parallel processes that will upload files. The **id_type** determines how the id will be generated from the CSV, it can either be specified manually based on the identifier column, hash of the row (combination of all data within the row), or generated randomly (`identifier`, `hash`, `random` as options).\n",
        "\n",
        "After doing this, upload your CSV within the files section to the left and change the CSV path to the name of your CSV in the **csv_path** below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yZo3SsxdzMH6"
      },
      "outputs": [],
      "source": [
        "# @title Script\n",
        "access_key = \"\" # @param {type:\"string\"}\n",
        "secret_key = \"\" # @param {type:\"string\"}\n",
        "workers = 1 # @param {type:\"integer\"}\n",
        "id_type = \"hash\" # @param [\"hash\", \"identifier\", \"random\"]\n",
        "skip = True # @param {type:\"boolean\"}\n",
        "csv_path = \"test.csv\" # @param {type:\"string\"}\n",
        "\"\"\"\n",
        "Uploads files to the Internet Archive based on a CSV containing file URLs and metadata.\n",
        "\n",
        "The main entry point is process_csv(), which takes a CSV path, IA keys, number of workers\n",
        "(concurrent downloads), ID type and handling existing uploads. First it reads the CSV, spawns\n",
        "threads to process each row, downloads the file, uploads it to IA, and logs the result. Helper\n",
        "functions handle the individual steps.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import configparser\n",
        "import csv\n",
        "import hashlib\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import tempfile\n",
        "import time\n",
        "from urllib.parse import quote, urlparse\n",
        "\n",
        "import requests\n",
        "from internetarchive import delete, get_item, get_session, upload\n",
        "from tqdm.notebook import tqdm\n",
        "from tqdm.contrib.concurrent import thread_map\n",
        "\n",
        "\n",
        "def configure_logging():\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    if logger.handlers:\n",
        "        logger.handlers.clear()\n",
        "\n",
        "    log_format = logging.Formatter(\n",
        "        \"%(asctime)s %(levelname)s: %(message)s\", datefmt=\"%H:%M:%S\"\n",
        "    )\n",
        "\n",
        "    file_handler = logging.FileHandler(\"log.txt\")\n",
        "    file_handler.setFormatter(log_format)\n",
        "\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setFormatter(log_format)\n",
        "\n",
        "    logger.addHandler(file_handler)\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "\n",
        "def load_session(config_file):\n",
        "    return get_session(config_file=config_file)\n",
        "\n",
        "\n",
        "def create_identifier(id_type, row=None):\n",
        "    if id_type == \"hash\":\n",
        "        row_str = \"\".join(str(value) for value in row.values())\n",
        "        hash_object = hashlib.md5(row_str.encode(), usedforsecurity=False)\n",
        "        identifier = hash_object.hexdigest()\n",
        "    else:\n",
        "        # 8 * 10^32 possibilities\n",
        "        identifier = \"\".join(random.choices(string.ascii_letters + string.digits, k=8))\n",
        "    return identifier\n",
        "\n",
        "\n",
        "def clean_metadata_text(text):\n",
        "    return text.replace(\"\\x00\", \"\")\n",
        "\n",
        "\n",
        "def clean_csv_data(csv_data):\n",
        "    cleaned_data = []\n",
        "    for row in csv_data:\n",
        "        cleaned_row = {k: clean_metadata_text(v) for k, v in row.items()}\n",
        "        cleaned_data.append(cleaned_row)\n",
        "    return cleaned_data\n",
        "\n",
        "\n",
        "def write_failed_url(url, file_path=\"failed.txt\"):\n",
        "    with open(file_path, \"a\") as f:\n",
        "        f.write(url + \"\\n\")\n",
        "\n",
        "\n",
        "def download_file_with_progress(url, output_path):\n",
        "    response = requests.get(url, stream=True, timeout=60)\n",
        "    total_size = int(response.headers.get(\"Content-Length\", 0))\n",
        "\n",
        "    with open(output_path, \"wb\") as f, tqdm(\n",
        "        desc=output_path,\n",
        "        total=total_size,\n",
        "        unit=\"B\",\n",
        "        unit_scale=True,\n",
        "        unit_divisor=1024,\n",
        "    ) as bar:\n",
        "        for data in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n",
        "            f.write(data)\n",
        "            bar.update(len(data))\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def encode_url(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    path = quote(parsed_url.path, safe=\"/+\")\n",
        "    return f\"{parsed_url.scheme}://{parsed_url.netloc}{path}\"\n",
        "\n",
        "\n",
        "def upload_to_internet_archive(file_path, metadata, keys, identifier):\n",
        "    try:\n",
        "        logging.info(f\"Uploading {os.path.basename(file_path)} to {identifier}.\")\n",
        "        upload(\n",
        "            identifier,\n",
        "            files=[file_path],\n",
        "            metadata=metadata,\n",
        "            access_key=keys[\"access_key\"],\n",
        "            secret_key=keys[\"secret_key\"],\n",
        "            retries=3,\n",
        "            retries_sleep=5,\n",
        "            verify=True,\n",
        "            delete=True\n",
        "        )\n",
        "        logging.info(\n",
        "            f\"Successfully uploaded {os.path.basename(file_path)} to {identifier}.\"\n",
        "        )\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to upload {os.path.basename(file_path)}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def process_row(row, keys, sleep=3, id_type=\"hash\", skip=True):\n",
        "    file_url = encode_url(row[\"file\"])\n",
        "    file_name = os.path.basename(file_url)\n",
        "    identifier = None\n",
        "\n",
        "    if id_type == \"identifier\":\n",
        "        identifier = row[\"identifier\"]\n",
        "    elif id_type == \"hash\":\n",
        "        # Use row to generate hash\n",
        "        identifier = create_identifier(id_type, row)\n",
        "    else:\n",
        "        identifier = create_identifier(id_type)\n",
        "\n",
        "    # Check if ID already exists\n",
        "    if get_item(identifier).exists:\n",
        "        if skip:\n",
        "            logging.info(f\"Skipping existing item: {row['identifier']}\")\n",
        "        else:\n",
        "            logging.error(f\"Item already exists: {row['identifier']}\")\n",
        "        return\n",
        "\n",
        "    logging.info(f\"Starting download for {file_name} from {file_url}\")\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(\n",
        "        delete=False, suffix=os.path.splitext(file_name)[1]\n",
        "    ) as temp_file:\n",
        "        local_file_path = temp_file.name\n",
        "\n",
        "    if download_file_with_progress(file_url, local_file_path):\n",
        "        logging.info(f\"Downloaded {file_name} to {local_file_path}\")\n",
        "\n",
        "        metadata = {\n",
        "            key: value\n",
        "            for key, value in row.items()\n",
        "            if key not in [\"identifier\", \"file\"]\n",
        "        }\n",
        "\n",
        "        if upload_to_internet_archive(local_file_path, metadata, keys, identifier):\n",
        "            if os.path.exists(local_file_path):\n",
        "                os.remove(local_file_path)\n",
        "                logging.info(f\"Removed local file {local_file_path}\")\n",
        "        else:\n",
        "            write_failed_url(file_url)\n",
        "    else:\n",
        "        write_failed_url(file_url)\n",
        "        os.remove(local_file_path)\n",
        "\n",
        "    time.sleep(random.uniform(0, sleep))\n",
        "\n",
        "\n",
        "def process_csv(csv_path, keys, id_type=\"hash\", skip=True, max_workers=3):\n",
        "    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        csv_data = list(csv.DictReader(csvfile))\n",
        "    \n",
        "    cleaned_data = clean_csv_data(csv_data)\n",
        "\n",
        "    thread_map(\n",
        "        lambda row: process_row(row, keys, id_type=id_type, skip=skip),\n",
        "        cleaned_data,\n",
        "        max_workers=max_workers\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    configure_logging()\n",
        "\n",
        "    keys = {\"access_key\": access_key, \"secret_key\": secret_key}\n",
        "\n",
        "    process_csv(\n",
        "        csv_path,\n",
        "        keys,\n",
        "        id_type=id_type,\n",
        "        skip=skip,\n",
        "        max_workers=workers)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM+9flrWE2XbhNT04GGJ8Xv",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
